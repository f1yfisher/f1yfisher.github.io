---
layout: post
title: 朴素贝叶斯法
date: 2021-08-21
author: zhaogs
header-img: img/post-bg-android.jpg
catalog: true
tags:
      - 读书笔记
      - 统计学习方法
---

## 一、朴素贝叶斯法

在概率论中我们曾经学习过贝叶斯公式，它是通过修正先验概率而得到后验概率，后验概率由于考虑到了新情况的加入，更加地接近实际情况。在实际应用中，朴素贝叶斯法凭借其学习与预测的高效率使其成为一种较为常用的方法。

假设输入空间$${\cal{X}}\subseteq R^n $$为$$n$$维向量的集合，输出空间为类标记集合$${\cal{Y}=\{c_1,c_2,...,c_k\}}$$。输入为特征向量$$x\in\cal X$$，输出为类标记$$y\in \cal Y$$。$$X$$为定义在输入空间$$\cal X$$上的随机变量，$$Y$$是定义在输出空间$$\cal Y$$上的随机变量。训练数据集为$$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$。

在分类问题中，我们可以获得样本的特征向量，然后预测样本的类别，因此我们想要得到的是一个

后验概率$$P(Y=c_k\vert X=x)$$，由于这个后验概率是不能直接通过样本的数据来计算得到，可以使用贝叶斯公式来进行计算得到，在使用贝叶斯公式时，我们首先必须要有先验概率$$P(Y=c_k)$$和条件概率$$P(X=x\vert Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...X^{(n)}=x^{(n)}\vert Y=c_k)$$。这两个概率是可以直接通过训练集样本进行计算得到的。

同时为进一步简化计算，朴素贝叶斯法对条件概率作了条件独立性假设，条件独立假设是说用于分类的特征在类确定的情况下都是条件独立的，其数学表达为


$$
\begin{aligned}
P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...X^{(n)}=x^{(n)}|Y=c_k)\\
&=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\end{aligned}
$$


因为是条件独立的所以可以将各项特征的条件概率相乘。

有了上述的先验概率和条件概率，依据贝叶斯公式可以得到其后验概率为


$$
\begin{aligned}
P(Y=c_k|X=x)&=\frac{P(Y=c_k)P(X=x|Y=c_k)}{\sum _{i=1}^KP(Y=c_i)P(X=x|Y=c_i)}\\
&=\frac{P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum _{i=1}^K P(Y=c_i)\prod _{j=1} ^n P(X^{(j)}=x^{(j)}|Y=c_i)}
\end{aligned}
$$


因此朴素贝叶斯法分类器可以表示为


$$
y=f(x)=\arg \max _{c_k}\frac{P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum _{i=1}^K P(Y=c_i)\prod _{j=1} ^n P(X^{(j)}=x^{(j)}|Y=c_i)}
$$


对于任意一个输入来说，分母是一个固定值，因此可以简化为


$$
y=f(x)=\arg\max_{c_k}P(Y=c_k)\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)
$$



## 二、朴素贝叶斯法的参数估计

### 2.1 极大似然估计

极大似然估计就是直接将样本中的频率作为概率来进行计算，假设输入的第$$j$$个特征的可能取值的集合为$$\{a_{j1},a_{j2},...a_{jl}\}$$,则先验概率和条件概率的数学表达为


$$
\begin{aligned}
P(Y=c_k)&=\frac{\sum_{i=1}^N I(y_i=c_k)}{N}\\
P(X^{(j)}=a_{jl}|Y=c_k) &= \frac{\sum_{i=1}^N I(x^{(j)}_i=a_{jl},y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}
\end{aligned}
$$



### 2.2 贝叶斯估计

使用极大似然法进行估计时可能会出现所要估计的概率值为0的情况，这会影响到后验概率的计算结果，解决这种问题的方法就是采用贝叶斯估计，其数学表达为


$$
\begin{aligned}
P_\lambda(Y=c_k)&=\frac{\sum_{i=1}^N I(y_i=c_k)+\lambda}{N+K\lambda}\\
P_\lambda(X^{(j)}=a_{jl}|Y=c_k) &= \frac{\sum_{i=1}^N I(x^{(j)}_i=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^N I(y_i=c_k)+S_j\lambda}
\end{aligned}
$$


其中$$K$$为$$c_k$$可能取值的个数，$$S_j$$为$$X^{(j)}$$可能取值的个数。当$$\lambda=1$$时为极大似然估计，一般取$$\lambda=1$$，这时称为拉普拉斯平滑。

