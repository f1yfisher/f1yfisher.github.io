---

layout:   post          # 使用的布局（不需要改）
title:   变分自编码器        # 标题 
date:    2021-05-24       # 时间
author:   zhaogs           # 作者
header-img: img/post-bg-coffee.jpeg  #这篇文章标题背景图片
catalog: true            # 是否归档
tags:                #标签
    - machine learning 
---

前段时间在做毕设的时候遇到了变分自编码器（Variational Auto-Encoder，VAE），网上找了许多教程进行了学习，有些教程讲的也有些不明不白，看得让人云里雾里，在看了一些博客，代码以及[原文](https://arxiv.org/pdf/1312.6114.pdf)后，有了自己的理解。

本文或许还存在着一些错误，希望能够得到读者的指正。

## 一、VAE概念

VAE的目的是将输入数据编码到隐空间，再将隐空间的数据解码到原数据空间。在实际使用中，为了完成不同的任务，在测试阶段也可以只使用单独的编码部分或解码部分。VAE的设计让模型在训练的过程中也存在一种对抗，与GAN网络相似，因此可以应用在无监督学习的任务中。

## 二、VAE原理

首先来看一组数据\$x=\{x_1,x_2,x_3,...,x_K\}\$，我们要训练一个模型将数据$$x$$转换到隐空间得到$$z$$，为便于理解，本文只考虑输入维度与编码维度相等的情况，即$$z=\{z_1,z_2,z_3,...,z_K\}$$，模型可以表示为$$q_\phi(z\vert x)$$，再将隐空间的数据解码到原始的数据空间得到重构的$$\widehat{x}=\{\widehat{x}_1,\widehat{x}_2,\widehat{x}_3,...,\widehat{x}_K\}$$，改模型可以表示为$$p_\theta(x\vert z)$$，之前在看博客和原文时对这个模型一直存在着比较大疑问，认为应该是$$p_\theta(\widehat{x}\vert z)$$，但是在博客和原文中都没有出现过这种写法，在反复对原文和博客的阅读中，理解了这种写法，在这里$$x$$与$$\widehat{x}$$都同属于原始数据空间，$$x$$也只是原始数据空间中的一些样本，不能表示原始空间的分布，若能直接表示原始数据空间的分布，那就不需要费劲地将原始数据转换到隐空间，再将隐空间的数据转换到原始数据空间。同时这里的模型输出的$$x$$与$$z$$表示的是一种分布，而最终输出的数据是对这两个分布采样得到的结果。

<img src="/img/VAE.png" alt="VAE" style="zoom:50%;" />

### 2.1 公式推导

假定所有数据都是独立同分布的，我们要对![](http://latex.codecogs.com/svg.latex?p_\theta(x|z))做参数估计，可以使用对数最大似然法，即最大化如下的似然函数：

<img src="https://latex.codecogs.com/svg.latex?\ln&space;p_\theta(x^{(1)},x^{(2)},x^{(3)},...,x^{(N)})=\sum_{i=1}^N\ln&space;p_\theta(x^{(i)})" title="\ln p_\theta(x^{(1)},x^{(2)},x^{(3)},...,x^{(N)})=\sum_{i=1}^N\ln p_\theta(x^{(i)})" />

然后在整个模型中需要令<img src="https://latex.codecogs.com/svg.latex?q_\phi(z|x^{(i)})" title="q_\phi(z|x^{(i)})" />逼近<img src="https://latex.codecogs.com/svg.latex?p_\theta(z|x^{(i)})" title="p_\theta(z|x^{(i)})" />，可以使用KL散度计算，即

<img src="https://latex.codecogs.com/svg.latex?\begin{aligned}&space;D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z|x^{(i)}))&space;&=&space;\int&space;q_\phi(z|x^{(i)})&space;\ln&space;\frac{q_\phi(z|x^{(i)})}{p_\theta(z|x^{(i)})}dz&space;\\&space;&=\mathbb{E}_{z&space;\sim&space;q_\phi(z|x^{(i)})}\ln&space;\frac{q_\phi(z|x^{(i)})}{p_\theta(z|x^{(i)})}\\&space;&=\mathbb{E}_{z&space;\sim&space;q_\phi(z|x^{(i)})}\ln&space;\frac{q_\phi(z|x^{(i)})p_\theta(x^{(i)})}{p_\theta(z,x^{(i)})}\\&space;&=\mathbb{E}_{z&space;\sim&space;q_\phi(z|x^{(i)})}\ln&space;\frac{q_\phi(z|x^{(i)})}{p_\theta(z,x^{(i)})}&space;&plus;\mathbb{E}_{z&space;\sim&space;q_\phi(z|x^{(i)})}\ln&space;p_\theta(x^{(i)})\\&space;&=\mathbb{E}_{z&space;\sim&space;q_\phi(z|x^{(i)})}\ln&space;\frac{q_\phi(z|x^{(i)})}{p_\theta(z,x^{(i)})}&plus;\ln&space;p_\theta(x^{(i)})&space;\end{aligned}" title="\begin{aligned} D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z|x^{(i)})) &= \int q_\phi(z|x^{(i)}) \ln \frac{q_\phi(z|x^{(i)})}{p_\theta(z|x^{(i)})}dz \\ &=\mathbb{E}_{z \sim q_\phi(z|x^{(i)})}\ln \frac{q_\phi(z|x^{(i)})}{p_\theta(z|x^{(i)})}\\ &=\mathbb{E}_{z \sim q_\phi(z|x^{(i)})}\ln \frac{q_\phi(z|x^{(i)})p_\theta(x^{(i)})}{p_\theta(z,x^{(i)})}\\ &=\mathbb{E}_{z \sim q_\phi(z|x^{(i)})}\ln \frac{q_\phi(z|x^{(i)})}{p_\theta(z,x^{(i)})} +\mathbb{E}_{z \sim q_\phi(z|x^{(i)})}\ln p_\theta(x^{(i)})\\ &=\mathbb{E}_{z \sim q_\phi(z|x^{(i)})}\ln \frac{q_\phi(z|x^{(i)})}{p_\theta(z,x^{(i)})}+\ln p_\theta(x^{(i)}) \end{aligned}" />

因此可以得到

<img src="https://latex.codecogs.com/svg.latex?\begin{aligned}&space;\ln&space;p_\theta(x^{(i)})&space;&=&space;D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z|x^{(i)}))&space;-\mathbb{E}_{z&space;\sim&space;q_\phi(z|x^{(i)})}\ln&space;\frac{q_\phi(z|x^{(i)})}{p_\theta(z,x^{(i)})}&space;\end{aligned}" title="\begin{aligned} \ln p_\theta(x^{(i)}) &= D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z|x^{(i)})) -\mathbb{E}_{z \sim q_\phi(z|x^{(i)})}\ln \frac{q_\phi(z|x^{(i)})}{p_\theta(z,x^{(i)})} \end{aligned}" />

由于KL散度是一个非负值，因此可以得到

<img src="https://latex.codecogs.com/svg.latex?\begin{aligned}&space;\ln&space;p_\theta(x^{(i)})&space;&\geq&space;-\mathbb{E}_{z&space;\sim&space;q_\phi(z|x^{(i)})}\ln&space;\frac{q_\phi(z|x^{(i)})}{p_\theta(z,x^{(i)})}\\&space;&=\mathbb{E}_{z&space;\sim&space;q_\phi(z|x^{(i)})}\ln&space;\frac{p_\theta(z,x^{(i)})}{q_\phi(z|x^{(i)})}\\&space;&=\int&space;q_\phi(z|x^{(i)})\ln&space;p_\theta(z,x^{(i)})dz&space;-&space;\int&space;q_\phi(z|x^{(i)})&space;\ln&space;q_\phi(z|x^{(i)})dz\\&space;\end{aligned}" title="\begin{aligned} \ln p_\theta(x^{(i)}) &\geq -\mathbb{E}_{z \sim q_\phi(z|x^{(i)})}\ln \frac{q_\phi(z|x^{(i)})}{p_\theta(z,x^{(i)})}\\ &=\mathbb{E}_{z \sim q_\phi(z|x^{(i)})}\ln \frac{p_\theta(z,x^{(i)})}{q_\phi(z|x^{(i)})}\\ &=\int q_\phi(z|x^{(i)})\ln p_\theta(z,x^{(i)})dz - \int q_\phi(z|x^{(i)}) \ln q_\phi(z|x^{(i)})dz\\ \end{aligned}" />

从这里就可以得到我们的损失函数

<img src="https://latex.codecogs.com/svg.latex?\begin{aligned}&space;\mathcal{L(\phi,\theta;x^{(i)})}&=\int&space;q_\phi(z|x^{(i)})&space;\ln&space;q_\phi(z|x^{(i)})dz&space;-\int&space;q_\phi(z|x^{(i)})&space;\ln&space;p_\theta(z,x^{(i)})dz\\&space;&=\int&space;q_\phi(z|x^{(i)})&space;\ln&space;q_\phi(z|x^{(i)})dz&space;-\int&space;q_\phi(z|x^{(i)})&space;\ln&space;p_\theta(x^{(i)}|z)p_\theta(z)dz\\&space;&=\int&space;q_\phi(z|x^{(i)})&space;\ln&space;\frac{q_\phi(z|x^{(i)})}{p_\theta(z)}dz&space;-\int&space;q_\phi(z|x^{(i)})&space;\ln&space;p_\theta(x^{(i)}|z)dz\\&space;&=D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z))-\mathbb{E}_{z&space;\sim&space;q_\phi(z|x^{(i)})}\ln&space;p_\theta(x^{(i)}|z)\\&space;\end{aligned}" title="\begin{aligned} \mathcal{L(\phi,\theta;x^{(i)})}&=\int q_\phi(z|x^{(i)}) \ln q_\phi(z|x^{(i)})dz -\int q_\phi(z|x^{(i)}) \ln p_\theta(z,x^{(i)})dz\\ &=\int q_\phi(z|x^{(i)}) \ln q_\phi(z|x^{(i)})dz -\int q_\phi(z|x^{(i)}) \ln p_\theta(x^{(i)}|z)p_\theta(z)dz\\ &=\int q_\phi(z|x^{(i)}) \ln \frac{q_\phi(z|x^{(i)})}{p_\theta(z)}dz -\int q_\phi(z|x^{(i)}) \ln p_\theta(x^{(i)}|z)dz\\ &=D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z))-\mathbb{E}_{z \sim q_\phi(z|x^{(i)})}\ln p_\theta(x^{(i)}|z)\\ \end{aligned}" />

对于隐空间中的数据分布，我们可以假定为任意的分布，因为我们可以将原始数据转换到一个我们想要的空间，因此在此处我们可以假定为高斯分布，对于假定为高斯分布的优势下面会进行叙述。当![](http://latex.codecogs.com/svg.latex?z)假定服从标准高斯分布可以得到

<img src="https://latex.codecogs.com/svg.latex?\begin{aligned}&space;p_\theta&space;(z)&space;&\sim&space;\mathcal{N}(0,1)\\&space;q_\phi(z|x_i)&space;&\sim&space;\mathcal{N}(\mu(x_i,\phi),\sigma&space;^2(x_i,\phi))&space;\end{aligned}" title="\begin{aligned} p_\theta (z) &\sim \mathcal{N}(0,1)\\ q_\phi(z|x_i) &\sim \mathcal{N}(\mu(x_i,\phi),\sigma ^2(x_i,\phi)) \end{aligned}" />

可以将损失函数![](http://latex.codecogs.com/svg.latex?\mathcal{L}(\phi,\theta;x^{(i)}))的第一项进一步地化简得到

<img src="https://latex.codecogs.com/svg.latex?\begin{aligned}&space;D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z))&=\mathbb{E}_{z&space;\sim&space;\mathcal{N}(\mu,\sigma&space;^2)}\ln&space;\mathcal{N}(\mu,\sigma&space;^2)&space;-&space;\mathbb{E}_{z&space;\sim&space;\mathcal{N}(\mu,\sigma&space;^2)}\ln&space;\mathcal{N}(0,1)\\&space;&=\mathbb{E}_{z&space;\sim&space;\mathcal{N},\sigma&space;^2)}\ln&space;{\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(z-\mu)^2}{2\sigma^2}}}-\mathbb{E}_{z\sim&space;\mathcal{N}(\mu,\sigma^2)}\ln&space;{\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}}\\&space;&=-\frac{1}{2}\ln&space;{2\pi}-\frac{1}{2}\ln&space;{\sigma^2}-\frac{1}{2\sigma^2}\mathbb{E}_{z\sim&space;\mathcal{N}(\mu,\sigma^2)}(z-\mu)^2&plus;\frac{1}{2}\ln2\pi&plus;\frac{1}{2}\mathbb{E}_{z\sim&space;\mathcal{N}(\mu,\sigma^2)}{z^2}\\&space;&=-\frac{1}{2}(\ln&space;\sigma^2&plus;1)&plus;\frac{1}{2}(\mu^2&plus;\sigma^2)\\&space;&=\frac{1}{2}\sum_{j=1}^{J}\mu_j^2(x^{(i)})&plus;\sigma_j^2(x^{(i)})-\ln&space;\sigma_j^2(x^{(i)})-1&space;\end{aligned}" title="\begin{aligned} D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z))&=\mathbb{E}_{z \sim \mathcal{N}(\mu,\sigma ^2)}\ln \mathcal{N}(\mu,\sigma ^2) - \mathbb{E}_{z \sim \mathcal{N}(\mu,\sigma ^2)}\ln \mathcal{N}(0,1)\\ &=\mathbb{E}_{z \sim \mathcal{N},\sigma ^2)}\ln {\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(z-\mu)^2}{2\sigma^2}}}-\mathbb{E}_{z\sim \mathcal{N}(\mu,\sigma^2)}\ln {\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}}\\ &=-\frac{1}{2}\ln {2\pi}-\frac{1}{2}\ln {\sigma^2}-\frac{1}{2\sigma^2}\mathbb{E}_{z\sim \mathcal{N}(\mu,\sigma^2)}(z-\mu)^2+\frac{1}{2}\ln2\pi+\frac{1}{2}\mathbb{E}_{z\sim \mathcal{N}(\mu,\sigma^2)}{z^2}\\ &=-\frac{1}{2}(\ln \sigma^2+1)+\frac{1}{2}(\mu^2+\sigma^2)\\ &=\frac{1}{2}\sum_{j=1}^{J}\mu_j^2(x^{(i)})+\sigma_j^2(x^{(i)})-\ln \sigma_j^2(x^{(i)})-1 \end{aligned}" />

其中![](http://latex.codecogs.com/svg.latex?\mathbb{E}_{z\sim \mathcal{N}(\mu,\sigma^2)}(z-\mu)^2)为方差![](http://latex.codecogs.com/svg.latex?\sigma^2)，![](http://latex.codecogs.com/svg.latex?\mathbb{E}_{z\sim \mathcal{N}(\mu,\sigma^2)}{z^2})为变量![](http://latex.codecogs.com/svg.latex?z)的二阶矩，结果是![](http://latex.codecogs.com/svg.latex?\mu^2+\sigma^2)。这里的![](http://latex.codecogs.com/svg.latex?\mu(x))与![](http://latex.codecogs.com/svg.latex?\sigma(x))可以通过神经网络直接求得。

对于损失函数![](http://latex.codecogs.com/svg.latex?\mathcal{L}(\phi,\theta;x^{(i)}))的第二项，首先需要了解蒙特卡洛模拟，此处只需要知道该理论的结论即可。即

<img src="https://latex.codecogs.com/svg.latex?\begin{aligned}&space;\mathbb{E}_{x\sim&space;p(x)}f(x)&space;&\approx&space;\frac{1}{L}\sum&space;_{i=1}^Lf(x_i)\quad&space;x_i&space;\sim&space;p(x)\\&space;\end{aligned}" title="\begin{aligned} \mathbb{E}_{x\sim p(x)}f(x) &\approx \frac{1}{L}\sum _{i=1}^Lf(x_i)\quad x_i \sim p(x)\\ \end{aligned}" />

由此可以得到损失函数![](http://latex.codecogs.com/svg.latex?\mathcal{L}(\phi,\theta;x^{(i)}))的最终结果为

<img src="https://latex.codecogs.com/svg.latex?\begin{aligned}&space;\mathcal{L}(\phi,\theta;x^{(i)})&space;&\approx&space;\widetilde{\mathcal{L}}(\phi,\theta;x^{(i)})&space;\\&space;&=\frac{1}{2}\sum_{j=1}^{J}\big[\mu_j^2(x^{(i)})&plus;\sigma_j^2(x^{(i)})-\ln&space;\sigma_j^2(x^{(i)})-1\big]-\frac{1}{L}\sum&space;_{l=1}^L\ln&space;p_\theta(x^{(i)}|z^{(i,l)})&space;\end{aligned}" title="\begin{aligned} \mathcal{L}(\phi,\theta;x^{(i)}) &\approx \widetilde{\mathcal{L}}(\phi,\theta;x^{(i)}) \\ &=\frac{1}{2}\sum_{j=1}^{J}\big[\mu_j^2(x^{(i)})+\sigma_j^2(x^{(i)})-\ln \sigma_j^2(x^{(i)})-1\big]-\frac{1}{L}\sum _{l=1}^L\ln p_\theta(x^{(i)}|z^{(i,l)}) \end{aligned}" />

其中<img src="https://latex.codecogs.com/svg.latex?z^{(i,l)}&space;\sim&space;q_\phi(z|x^{(i)})" title="z^{(i,l)} \sim q_\phi(z|x^{(i)})" />

所以总的损失函数![](http://latex.codecogs.com/svg.latex?\mathcal{L}(\phi,\theta;x))为

<img src="https://latex.codecogs.com/svg.latex?\mathcal{L}(\phi,\theta;x)=\sum_{i=1}^{N}\mathcal{\widetilde{L}}(\phi,\theta;x^{(i)})" title="\mathcal{L}(\phi,\theta;x)=\sum_{i=1}^{N}\mathcal{\widetilde{L}}(\phi,\theta;x^{(i)})" />

因此要求最大话似然函数，即最小化总损失函数![](http://latex.codecogs.com/svg.latex?\mathcal{L}(\phi,\theta;x))。

在实际应用中，由于数据量![](http://latex.codecogs.com/svg.latex?N)非常大，因此需要将数据分为![](http://latex.codecogs.com/svg.latex?M)个minibatch进行训练，优化后的损失函数为

<img src="https://latex.codecogs.com/svg.latex?\mathcal{L}(\phi,\theta;x)&space;=&space;\frac{N}{M}\sum_{i=1}^{M}\mathcal{\widetilde&space;L}(\phi,\theta;x^{(i)})" title="\mathcal{L}(\phi,\theta;x) = \frac{N}{M}\sum_{i=1}^{M}\mathcal{\widetilde L}(\phi,\theta;x^{(i)})" />

当![](http://latex.codecogs.com/svg.latex?M)比较大时，内层估计可以由外层估计来完成，即对于每一个样本来说，内层估计需要从<img src="https://latex.codecogs.com/svg.latex?q_\phi(z|x^{(i)})" title="q_\phi(z|x^{(i)})" />中采样![](http://latex.codecogs.com/svg.latex?L)个数据来进行估计，当样本量比较大时，内层的估计可以忽略，即![](http://latex.codecogs.com/svg.latex?L=1)，这是因为样本量大到一定程度，每个样本的多次采样对结果的影响不会太大，因此每个样本只需要采样一个数据即可。在该[论文](https://arxiv.org/pdf/1312.6114.pdf)的实验中，取![](http://latex.codecogs.com/svg.latex?M=100, L=1)。

### 2.2 生成模型的估计

至此，我们可以得到VAE模型的目标函数，但是生成模型还需要选择一个分布，在[论文](https://arxiv.org/pdf/1312.6114.pdf)中给出了两种候选方案，伯努利分布和正态分布。

#### (a) 伯努利分布

伯努利分布的随机变量![](http://latex.codecogs.com/svg.latex?x)只能取两个值0或1，其概率函数为

<img src="https://latex.codecogs.com/svg.latex?p(x|\rho)=&space;\begin{cases}&space;\rho^x(1-\rho)^{1-x}&space;&&space;x=0,1\\&space;0&space;&&space;x\neq&space;0,1&space;\end{cases}" title="p(x|\rho)= \begin{cases} \rho^x(1-\rho)^{1-x} & x=0,1\\ 0 & x\neq 0,1 \end{cases}" />

对于多元二值向量，生成模型<img src="https://latex.codecogs.com/svg.latex?p_\theta(x^{(i)}|z)" title="p_\theta(x^{(i)}|z)" />的概率函数为

<img src="https://latex.codecogs.com/svg.latex?p_\theta(x^{(i)}|z)=\prod_{k=1}^{K}\rho_k^{x_k^{(i)}}(z)(1-\rho_k(z))^{1-x_k^{(i)}}" title="p_\theta(x^{(i)}|z)=\prod_{k=1}^{K}\rho_k^{x_k^{(i)}}(z)(1-\rho_k(z))^{1-x_k^{(i)}}" />

这里的![](http://latex.codecogs.com/svg.latex?\rho^{x^{(i)}}(z))就是相当于一个decoder，将隐空间的数据解码到数据空间。

因此损失函数中的最后一项可以化简为

<img src="https://latex.codecogs.com/svg.latex?\ln&space;p_\theta(x^{(i)}|z)=\sum_{k=1}^{K}&space;\big[x_k^{(i)}\ln\rho_k(z)&plus;({1-x_k^{(i)}})(1-\rho_k(z))\big]" title="\ln p_\theta(x^{(i)}|z)=\sum_{k=1}^{K} \big[x_k^{(i)}\ln\rho_k(z)+({1-x_k^{(i)}})(1-\rho_k(z))\big]" />

#### (b) 正态分布

当生成模型为正态分布时，生成模型的概率函数为

<img src="https://latex.codecogs.com/svg.latex?p_\theta(x^{(i)}|z)=\frac{1}{&space;\prod_{k=1}^K&space;\sqrt&space;{&space;2&space;\pi&space;\widetilde\sigma^2_k(z)&space;}&space;}&space;e^{-\frac{1}{2}&space;\big\lVert{\frac{x^{(i)}-\widetilde\mu(z)}{\widetilde\sigma(z)}}\big\rVert^2}" title="p_\theta(x^{(i)}|z)=\frac{1}{ \prod_{k=1}^K \sqrt { 2 \pi \widetilde\sigma^2_k(z) } } e^{-\frac{1}{2} \big\lVert{\frac{x^{(i)}-\widetilde\mu(z)}{\widetilde\sigma(z)}}\big\rVert^2}" />

这里的![](http://latex.codecogs.com/svg.latex?\widetilde\mu(z))与![](http://latex.codecogs.com/svg.latex?\widetilde\sigma(z))均可以使用神经网络得到，但是我们往往想要得到一个确定的结果，因此我们会将这里的方差![](http://latex.codecogs.com/svg.latex?\widetilde\sigma(z))为固定为一个常数，均值![](http://latex.codecogs.com/svg.latex?\widetilde\mu(z))相当于一个decoder。则损失函数的最后一项可以化简为

<img src="https://latex.codecogs.com/svg.latex?\begin{aligned}&space;\ln&space;p_\theta(x^{(i)}|z)&=-\frac{K}{2}\ln&space;2\pi-\frac{1}{2}\sum_{k=1}^K\ln&space;\widetilde\sigma_k^2(z)&space;-\frac{1}{2}\big\lVert\frac{x^{(i)}-\widetilde\mu(z)}{\widetilde\sigma(z)}\big\rVert^2\\&space;&\simeq&space;-\lVert{x^{(i)}-\widetilde\mu(z)}\rVert^2&space;\end{aligned}" title="\begin{aligned} \ln p_\theta(x^{(i)}|z)&=-\frac{K}{2}\ln 2\pi-\frac{1}{2}\sum_{k=1}^K\ln \widetilde\sigma_k^2(z) -\frac{1}{2}\big\lVert\frac{x^{(i)}-\widetilde\mu(z)}{\widetilde\sigma(z)}\big\rVert^2\\ &\simeq -\lVert{x^{(i)}-\widetilde\mu(z)}\rVert^2 \end{aligned}" />

### 2.3 重参数技巧

在生成模型![](http://latex.codecogs.com/svg.latex?p_\theta(x|z))中，![](http://latex.codecogs.com/svg.latex?z)是通过对识别模型![](http://latex.codecogs.com/svg.latex?q_\phi(z|x))采样得到的，然而采样的操作是不可导的，这使得直接采样不能使用链式求导法则进行求导，所有的信息反馈到隐空间就断了，无法反馈到识别模型![](http://latex.codecogs.com/svg.latex?q_\phi(z|x))，不能对![](http://latex.codecogs.com/svg.latex?\mu(x))和![](http://latex.codecogs.com/svg.latex?\sigma(x))的参数进行修正。为解决这个问题，采用了重参数的技巧，即![](http://latex.codecogs.com/svg.latex?z^{(i,l)})通过如下公式计算得到

<img src="https://latex.codecogs.com/svg.latex?z^{(i,l)}=\mu(x^{(i)})&plus;\sigma(x^{(i)})\cdot\epsilon^{(l)}&space;\quad\epsilon^{(l)}&space;\sim&space;\mathcal{N}(0,1)" title="z^{(i,l)}=\mu(x^{(i)})+\sigma(x^{(i)})\cdot\epsilon^{(l)} \quad\epsilon^{(l)} \sim \mathcal{N}(0,1)" />

通过对变量![](http://latex.codecogs.com/svg.latex?\epsilon)进行采样，避免了直接采样![](http://latex.codecogs.com/svg.latex?z)引发的无法采用梯度下降法学习的问题。采用这种方式，对于模型![](http://latex.codecogs.com/svg.latex?\mu(x))与![](http://latex.codecogs.com/svg.latex?\sigma(x))来说，![](http://latex.codecogs.com/svg.latex?\epsilon)只是一个常数，可以应用梯度下降法。

### 总结

在VAE模型中，需要构建三个神经网络，分别是![](http://latex.codecogs.com/svg.latex?\mu(x),\sigma(x))与![](http://latex.codecogs.com/svg.latex?\widetilde\mu(z))，其中![](http://latex.codecogs.com/svg.latex?\mu(x),\sigma(x))构成了识别模型![](http://latex.codecogs.com/svg.latex?q_\phi(z|x))，即为编码过程，![](http://latex.codecogs.com/svg.latex?\widetilde\mu(z))构成了生成模型![](http://latex.codecogs.com/svg.latex?p_\theta(x|z))，即为解码过程。

以生成模型为正态分布为例，其损失函数为

<img src="https://latex.codecogs.com/svg.latex?\widetilde{\mathcal{L}}(\phi,\theta;x^{(i)})&space;=\frac{1}{2}\sum_{j=1}^{J}\big[\mu_j^2(x^{(i)})&plus;\sigma_j^2(x^{(i)})-\ln&space;\sigma_j^2(x^{(i)})-1\big]&plus;\lVert{x^{(i)}-\widetilde\mu(z)}\rVert^2" title="\widetilde{\mathcal{L}}(\phi,\theta;x^{(i)}) =\frac{1}{2}\sum_{j=1}^{J}\big[\mu_j^2(x^{(i)})+\sigma_j^2(x^{(i)})-\ln \sigma_j^2(x^{(i)})-1\big]+\lVert{x^{(i)}-\widetilde\mu(z)}\rVert^2" />

这里的第一项为KL散度损失，第二项为重构损失。

不考虑KL散度损失，模型就退化为了普通的自动编码器（Auto-Encoder，AE）。在VAE中，重构损失希望生成的数据与原始数据相等，但是KL散度损失会使得隐空间的数据呈标准正态分布，在生成隐空间数据![](http://latex.codecogs.com/svg.latex?z)时，都会对数据进行采样，这会导致对于同一个输入数据，生成的隐空间数据都会不同，进一步地也就会在生成的数据中引入噪声。因此重构损失与KL散度损失是一种对抗的过程，保证了模型的生成数据接近原始数据，同时还增加了模型的泛化能力。

